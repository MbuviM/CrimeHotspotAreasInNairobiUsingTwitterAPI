{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# To obtain the API requests\n",
    "import requests\n",
    "# For data cleaning and display\n",
    "import pandas as pd\n",
    "# For file management and saving of API Tokens\n",
    "import os\n",
    "# To access the JSON file that contains the tweets\n",
    "import json\n",
    "# For parsing the time in readable format\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "# To record the time in which the tweets were obtained.\n",
    "import time\n",
    "import unicodedata\n",
    "# For CSV format files\n",
    "import csv\n",
    "from dotenv import load_dotenv, find_dotenv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 3\n",
      "Python-dotenv could not parse statement starting at line 4\n",
      "Python-dotenv could not parse statement starting at line 5\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the environmental variable file to parse the API Tokens\n",
    "load_dotenv(find_dotenv('Tokens.env'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing the API Bearer Token\n",
    "print(os.getenv('BEARER_TOKEN'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# Creating headers that will be used to access the APIs\n",
    "def headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "    return headers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Creating the URL request\n",
    "def CreateURL(keyword, start_date, end_date, max_results = 100):\n",
    "\n",
    "    # Returns the tweets in JSON format\n",
    "    Search_URL = \"https://api.twitter.com/1.1/search/tweets.json\"\n",
    "\n",
    "    query_params = {\n",
    "        'query': \"theft, wezi, wizi, dungwa, visu, killed, robbed\",\n",
    "        'lang': \"en, sw\",\n",
    "        'result_type' : \"mixed\",\n",
    "        'count' : max_results,\n",
    "        'start_date' : \"2022-11-06\",\n",
    "        'end_date' : \"2022-11-18\",\n",
    "        'place_fields' : 'Kenya',\n",
    "        'next_token' : {}\n",
    "    }\n",
    "\n",
    "    return(Search_URL, query_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2644904267.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn [61], line 12\u001B[1;36m\u001B[0m\n\u001B[1;33m    json_response = connect_to_endpoint(url[0], headers(bearer_token=), url[1])\u001B[0m\n\u001B[1;37m                                                                     ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Connecting the header, URL and parameters to the endpoint\n",
    "def connect_to_endpoint(url, header, param, Next_Token = None):\n",
    "    param['next_token'] = 'next_token'\n",
    "    responses = requests.request('GET', url, headers = header, params = param)\n",
    "    print(\"Endpoint Response Code: \" +str(responses.status_code))\n",
    "    if responses.status_code != 200:\n",
    "        raise Exception (responses.status_code, responses.text)\n",
    "\n",
    "    return responses.json\n",
    "# Parsing the responses in JSON readable format\n",
    "url = CreateURL('keyword', 'start_date', 'end_date', max_results = 100)\n",
    "json_response = connect_to_endpoint(url[0], headers(), url[1])\n",
    "print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "json_response['data'][0]['created_at']\n",
    "json_response['meta']['result_count']\n",
    "\n",
    "# Saving the responses as a CSV file\n",
    "df = pd.DataFrame(responses['json_response'])\n",
    "df.to_csv('data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "\n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):\n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "\n",
    "        # Assemble all data in a list\n",
    "        res = [created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "\n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter)\n",
    "\n",
    "append_to_csv(json_response, \"data.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}